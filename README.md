# Word-Embeddings (Greek Language)
The size of the corpus that I used to training the word embeddings is around 16Gb and consists of different news articles. 

# Word2Vec
Python implementation of Word2Vec with Gensim
The parameters of the Word2Vec that we did the training are:
⋅⋅* Unordered sub-list

# FastText
Python implementation of FastText with Gensim
